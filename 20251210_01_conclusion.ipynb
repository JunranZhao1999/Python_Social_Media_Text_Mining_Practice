{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f0ff-0969-429a-aa00-fb000a22f66b",
   "metadata": {},
   "source": [
    "#### BERT 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212974cb-7f18-43e9-9b66-b86ce4fb4139",
   "metadata": {},
   "source": [
    "使用 PyTorch 框架和 Hugging Face Transformers 库来获取中文句子 \"我爱秋天的颐和园\" 的 BERT 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a4ac0-bc89-4f0b-9c75-4b12b8702937",
   "metadata": {},
   "source": [
    "##### 5\\. 提取词向量\n",
    "\n",
    "```python\n",
    "# 提取词向量（最后一层的隐藏状态）\n",
    "# 最后一层（Last Layer）：包含最丰富的语境信息，但可能过拟合特定任务。\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "```\n",
    "\n",
    "  * `outputs` 是一个包含多个元素的元组/对象。\n",
    "  * **`outputs.last_hidden_state`**：提取模型最后一层（在 BERT Base 中是第 12 层）所有 token 的隐藏状态输出。这是 BERT 提取的**最终、上下文感知**的词向量。\n",
    "\n",
    "| 维度 | 含义 |\n",
    "| :--- | :--- |\n",
    "| **`last_hidden_states.shape[0]`** | Batch Size (此处为 1) |\n",
    "| **`last_hidden_states.shape[1]`** | 序列长度 (L) |\n",
    "| **`last_hidden_states.shape[2]`** | 隐藏层维度 (H=768) |\n",
    "\n",
    "##### 6\\. 输出结果\n",
    "\n",
    "```python\n",
    "# 输出结果\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "for token, vector in zip(tokens, last_hidden_states[0]):\n",
    "    print(f\"Token: {token} | Vector Shape: {vector.shape}\")\n",
    "```\n",
    "\n",
    "  * **`tokenizer.convert_ids_to_tokens(...)`**：将模型的数字输入 ID 转换回可读的 token 字符串。\n",
    "  * 循环遍历每个 token 及其对应的词向量，并打印结果。\n",
    "      * **分词结果**：`我爱秋天的颐和园` 会被分词为 `['[CLS]', '我', '爱', '秋', '天', '的', '颐', '和', '园', '[SEP]']`。\n",
    "      * **向量形状**：每个 token 都有一个 `torch.Size([768])` 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cd1db0-5e3b-47da-86f9-db6e82ccd9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 1b8de55f-b5fe-4de0-9d6d-09a3ba2338c6)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: caf21ec2-a803-4e15-b029-2c1c1c18eb15)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 0b07833b-64c1-4967-bb49-20b0b5bb11de)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 2894fae7-bd05-4f48-b43c-b14d6dadd319)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 1a91a20b-509d-409f-89df-86eeff989a34)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: ad817ef6-6984-463a-8713-fbd2d40c35d0)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 4e502686-f909-40de-85bd-63d61f62807e)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 7c5e6bc3-3521-425c-ae70-3b0a7abc9bbd)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 0628ce82-abb1-471c-b620-cc3ea4cb559f)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 7f747b25-7b5b-4599-a6b4-6cdd2c820c79)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 90a5ae25-24ae-4737-959a-ccc545a2f9cb)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: cfbeb234-a537-4346-95d5-0060d3a0503e)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] | Vector Shape: torch.Size([768])\n",
      "Token: 我 | Vector Shape: torch.Size([768])\n",
      "Token: 爱 | Vector Shape: torch.Size([768])\n",
      "Token: 秋 | Vector Shape: torch.Size([768])\n",
      "Token: 天 | Vector Shape: torch.Size([768])\n",
      "Token: 的 | Vector Shape: torch.Size([768])\n",
      "Token: 颐 | Vector Shape: torch.Size([768])\n",
      "Token: 和 | Vector Shape: torch.Size([768])\n",
      "Token: 园 | Vector Shape: torch.Size([768])\n",
      "Token: [SEP] | Vector Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "\n",
    "# 加载中文 BERT 预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 句子输入\n",
    "sentence = '我爱秋天的颐和园'\n",
    "\n",
    "# 进行分词并转换为输入 ID\n",
    "inputs = tokenizer(sentence,return_tensors = 'pt')\n",
    "\n",
    "# 前向传播获取 BERT 输出\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 提取词向量（最后一层的隐藏状态）\n",
    "# 最后一层（Last Layer）：包含最丰富的语境信息，但可能过拟合特定任务。\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "# outputs.last_hidden_state：提取模型最后一层（在 BERT Base 中是第 12 层）所有 token 的隐藏状态输出。这是 BERT 提取的最终、上下文感知的词向量。\n",
    "\n",
    "# 输出结果\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]) \n",
    "# tokenizer.convert_ids_to_tokens(...)：将模型的数字输入 ID 转换回可读的 token 字符串。\n",
    "for token,vector in zip(tokens,last_hidden_states[0]): \n",
    "    # last_hidden_states[0]: 由于 Batch Size 为 1，我们使用 [0] 索引来获取批次中的第一个（也是唯一一个）句子的输出张量。它的形状是 [10, 768]。\n",
    "    print(f\"Token: {token} | Vector Shape: {vector.shape}\")\n",
    "# [CLS] 和 [SEP] 是 BERT 特殊标记，[CLS] 可以用于句子级别的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7478b-df2e-4ebf-b4f9-bd0fc961fd9b",
   "metadata": {},
   "source": [
    "#### 余弦相似度 (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251838c4-6ba9-4952-b857-e0ae1b861ab3",
   "metadata": {},
   "source": [
    "使用 TF-IDF (Term Frequency-Inverse Document Frequency) 结合 余弦相似度 (Cosine Similarity) 来计算两个中文短文本相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d81609-987d-4c5a-aa48-2f41965e9bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list for tfidf\n",
    "texts = [\"我喜欢秋天的颐和园\", \"我每到秋天就会去颐和园\"] \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 1. 提取第一个句子 (tfidf_matrix[0]) 和第二个句子 (tfidf_matrix[1]) 的向量。2. 计算这两个 $N$ 维向量的余弦相似度。\n",
    "similarity = cosine_similarity(tfidf_matrix[0],tfidf_matrix[1]) # 值越接近1，相似度越高\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4b408-efb1-4327-a7f1-23bef870b820",
   "metadata": {},
   "source": [
    "#### Word2Vec 模型结合余弦相似度来计算两个中文句子相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ac4895-650c-4fb0-b76e-2f34eb9d1e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7186865]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sentences = [[\"我\", \"喜欢\", \"秋天\",\"颐和园\"], [\"我\", \"每到\",\"秋天\", \"会去\", \"颐和园\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, min_count=1)\n",
    "\n",
    "# 计算文本向量（取词向量均值）\n",
    "def get_avg_vector(text):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "vec1 = get_avg_vector([\"我\", \"喜欢\", \"秋天\",\"颐和园\"])\n",
    "vec2 = get_avg_vector([\"我\",  \"每到\",\"秋天\", \"会去\", \"颐和园\"])\n",
    "\n",
    "similarity = cosine_similarity([vec1], [vec2])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b09b9-d288-49ea-9366-99518910b76b",
   "metadata": {},
   "source": [
    "#### BERT 句向量（Sentence Embedding）和余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e087b0b7-79a2-43b8-92d8-3abb40ba7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 501ee956-0a14-4897-b05e-696a45cd2151)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 3c1fef3c-4c31-45bb-ac70-7b9d4b8210c5)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: fd7b19c5-8ff9-42c8-ba8c-3ad7724d64e8)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: e8dde29e-5cd1-438c-be2b-f6e439242c91)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 66d2e103-cfb3-4cd2-bf6d-25cbaef94dc2)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 2722da1e-0de4-4e8e-87d8-7f3a65e64066)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 75cc1c9d-9dde-4f3c-95ad-72aa5791e601)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.89764786]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text,\n",
    "                       return_tensors = 'pt',\n",
    "                       padding = True,\n",
    "                       truncation = True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:,0,:].numpy() # 取 [CLS] 位置的向量\n",
    "    # .numpy(): 将 PyTorch Tensor 转换为 NumPy 数组，以供 sklearn 的 cosine_similarity 函数使用。\n",
    "\n",
    "vec1 = get_bert_embedding(\"我喜欢秋天的颐和园\")\n",
    "vec2 = get_bert_embedding(\"我每到秋天就会去颐和园\")\n",
    "\n",
    "similarity = cosine_similarity(vec1,vec2)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f2b8e-d913-4163-9356-2b0c6618580f",
   "metadata": {},
   "source": [
    "#### Hugging Face Transformers 库执行命名实体识别 (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b216f4dd-f265-4530-818b-f5036570ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: c0f74bc1-a8a7-429c-9b28-0d3c23f756cc)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 10b57497-14d1-4f5e-a816-72c57dec6d3a)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: e0d5fe78-21c3-4411-b789-5b36cab48064)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 4ab53b69-d3fd-40ec-b6c2-9c32fa74d367)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: e8dd795c-449c-429f-b72d-381448d72973)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 42948d45-48dd-468a-9175-fb85a1985cd8)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/tokenizer_config.json\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: d0bcc424-6218-44a5-acbf-3af2a38465a7)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: ba36cb16-f19a-48a5-861f-0f9a5fa3244f)')' thrown while requesting HEAD https://huggingface.co/hfl/chinese-roberta-wwm-ext/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3d4525da19461c88738f834f55806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62aa9031ef7436cb3b85cfabab922e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576f1423f3504896bc1e9c6e134fd66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b796cccf2f5469b814c0bd7566cb535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a6f4cc01c949aa8a8f8be921bd8435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ba7e3dc89d445e8c8f181e77bb3203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5e387194ef46668999725fd481967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_1', 'score': 0.6227718, 'index': 1, 'word': '我', 'start': 0, 'end': 1}\n",
      "{'entity': 'LABEL_0', 'score': 0.50533104, 'index': 2, 'word': '每', 'start': 1, 'end': 2}\n",
      "{'entity': 'LABEL_0', 'score': 0.69991153, 'index': 3, 'word': '到', 'start': 2, 'end': 3}\n",
      "{'entity': 'LABEL_0', 'score': 0.51139605, 'index': 4, 'word': '秋', 'start': 3, 'end': 4}\n",
      "{'entity': 'LABEL_0', 'score': 0.64497894, 'index': 5, 'word': '天', 'start': 4, 'end': 5}\n",
      "{'entity': 'LABEL_1', 'score': 0.5511807, 'index': 6, 'word': '就', 'start': 5, 'end': 6}\n",
      "{'entity': 'LABEL_0', 'score': 0.5219732, 'index': 7, 'word': '会', 'start': 6, 'end': 7}\n",
      "{'entity': 'LABEL_0', 'score': 0.50868607, 'index': 8, 'word': '去', 'start': 7, 'end': 8}\n",
      "{'entity': 'LABEL_1', 'score': 0.5624909, 'index': 9, 'word': '颐', 'start': 8, 'end': 9}\n",
      "{'entity': 'LABEL_1', 'score': 0.51863146, 'index': 10, 'word': '和', 'start': 9, 'end': 10}\n",
      "{'entity': 'LABEL_1', 'score': 0.6211865, 'index': 11, 'word': '园', 'start': 10, 'end': 11}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# 加载中文 BERT 预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = AutoModelForTokenClassification.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 使用 pipeline 进行 NER 任务\n",
    "ner_pipeline = pipeline('ner',model = model,tokenizer = tokenizer)\n",
    "\n",
    "# 进行 NER 预测\n",
    "text = '我每到秋天就会去颐和园'\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# 输出识别的实体\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23fca587-a32e-470b-80f6-95f0220ac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "weibo_df = pd.read_excel(\"weibo_data.xlsx\") # read_csv\n",
    "data = []\n",
    "for i in range(1,len(weibo_df)):\n",
    "    data.append(weibo_df.iloc[i,3])\n",
    "    \n",
    "# 加载中文预训练模型\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "output = {\"data\": []}  # 创建一个字典，并准备好 data 键\n",
    "\n",
    "for line in data:\n",
    "    # 处理文本\n",
    "    doc = nlp(line)\n",
    "    for ent in doc.ents:\n",
    "        # 将每个实体对象直接添加到 \"data\" 数组中\n",
    "        output[\"data\"].append({\"实体\": ent.text, \"类型\": ent.label_})\n",
    "\n",
    "# 保存到 JSON 文件\n",
    "with open('entities_output1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b0afcc-8693-43c4-aed6-0843b9d352a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/hx/vrwsgqgx7bncbt28n44f0b540000gn/T/jieba.cache\n",
      "Loading model cost 0.485 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 示例数据\n",
    "corpus = [\n",
    "    \"这部电影很好看，我非常喜欢。\",\n",
    "    \"剧情很感人，演员演技也很棒。\",\n",
    "    \"太无聊了，浪费时间。\",\n",
    "    \"这是一部烂片，后悔看了。\",\n",
    "    \"非常精彩的故事，推荐观看。\",\n",
    "    \"不值得看，情节太差了。\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0]  # 1 代表好评，0 代表差评\n",
    "\n",
    "\n",
    "# 中文分词\n",
    "corpus_cut = [\" \".join(jieba.cut(text)) for text in corpus]\n",
    "\n",
    "# TF-IDF 特征提取\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eaf79ef-9e65-4adf-a763-4f97fa26bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 864 candidates, totalling 1728 fits\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END bootstrap=False, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "SVM最佳参数: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "SVM最佳模型: SVC(C=0.1, kernel='linear')\n",
      "RF最佳参数: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "RF最佳模型: RandomForestClassifier(n_estimators=10, random_state=42)\n",
      "SVM 预测结果: [1]\n",
      "Random Forest 预测结果: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "576 fits failed out of a total of 1728.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "305 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "271 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "svm = SVC()\n",
    "# 定义模型\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 定义超参数搜索空间\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],  # 森林中树的数量\n",
    "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
    "    'min_samples_split': [2, 5, 10],  # 每个节点最小的样本数\n",
    "    'min_samples_leaf': [1, 2, 4],  # 每个叶子节点最小的样本数\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # 在每个节点分裂时考虑的特征数\n",
    "    'bootstrap': [True, False]  # 是否使用自助采样法\n",
    "}\n",
    "\n",
    "# 定义超参数搜索空间\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],  # 惩罚参数\n",
    "    'kernel': ['linear', 'rbf'],  # 核函数\n",
    "    'gamma': ['scale', 'auto']  # 核函数的参数\n",
    "}\n",
    "\n",
    "# 创建 GridSearchCV 对象\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=2, scoring='accuracy')\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=2, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# 执行网格搜索\n",
    "grid_search_svm.fit(X, labels)\n",
    "grid_search_rf.fit(X, labels)\n",
    "\n",
    "# 输出最佳参数和最佳模型\n",
    "print(\"SVM最佳参数:\", grid_search_svm.best_params_)\n",
    "print(\"SVM最佳模型:\", grid_search_svm.best_estimator_)\n",
    "print(\"RF最佳参数:\", grid_search_rf.best_params_)\n",
    "print(\"RF最佳模型:\", grid_search_rf.best_estimator_)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "test_text = \"这部电影真的太棒了！\"\n",
    "test_vector = vectorizer.transform([\" \".join(jieba.cut(test_text))])\n",
    "print(\"SVM 预测结果:\", grid_search_svm.predict(test_vector))\n",
    "print(\"Random Forest 预测结果:\", grid_search_rf.predict(test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3de275-1ae3-4f74-aefe-4c9746d1ceec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
